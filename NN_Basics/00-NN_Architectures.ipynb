{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architectures\n",
    "\n",
    "## Modeling one neuron\n",
    "\n",
    "**Neuron**:\n",
    "\n",
    "‚Äã\t<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/neuron.png\" alt=\"img\" style=\"zoom:67%;\" />\n",
    "\n",
    "- Basic computational unit of the brain\n",
    "\n",
    "- Approximately 86 billion neurons in the human nervous system\n",
    "\n",
    "- Connected with approximately $10^{14}$ - $10^{15}$ **synapses**\n",
    "\n",
    "- Receives input signals from its **dendrites** and produces output signals along its (single) **axon**\n",
    "\n",
    "- The axon eventually branches out and connects via synapses to dendrites of other neurons\n",
    "\n",
    "  \n",
    "\n",
    "**Computational model**:\n",
    "\n",
    "‚Äã\t<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/neuron_model-20200516112329530.jpeg\" alt=\"img\" style=\"zoom:67%;\" />\n",
    "\n",
    "- The signals that travel along the axons (e.g. $x_0$) interact multiplicatively (e.g. $w_0x_0$) with the dendrites of the other neuron based on the synaptic strength at that synapse (e.g. $x_0$).\n",
    "\n",
    "- Synaptic strengths (the weights $w$)\n",
    "  - **Learnable** and control the strength of influence (and its direction: excitory (positive weight) or inhibitory (negative weight)) of one neuron on another\n",
    "\n",
    "- In the basic model, the dendrites carry the signal to the cell body where they all get **summed**\n",
    "  - Sum > a certain threshold $\\rightarrow$ the neuron can *fire*\n",
    "  - Model the firing rate of the neuron with **activation function $f$**\n",
    "    - Historically common choice: **sigmoid function $\\sigma$**\n",
    "      - $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "      - It takes a real-valued input (the signal strength after the sum) and squashes it to range between 0 and 1\n",
    "\n",
    "**In other words, each neuron performs a dot product with the input and its weights, adds the bias and applies the non-linearity (or activation function)**\n",
    "\n",
    "An example code for simple forward-propagating a single neuron:\n",
    "\n",
    "~~~python\n",
    "import numpy as np\n",
    "\n",
    "class Neuron(object):\n",
    "  \n",
    "  #...\n",
    "  \n",
    "  def forward(self, input):\n",
    "    \"\"\" assume inputs and weights are 1-D numpy arrays and bias is a number \"\"\"\n",
    "    cell_body_sum = np.sum(inputs * self.weights) + self.bias\n",
    "    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid\n",
    "    return firing_rate\n",
    "~~~\n",
    "\n",
    "### Commonly used activation functions\n",
    "\n",
    "#### Sigmoid\n",
    "\n",
    "![img](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/sigmoid.jpeg)\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "- Derivation: $\\sigma'(x)=\\sigma(x)(1-\\sigma(x))$\n",
    "\n",
    "- Takes a real-valued number and ‚Äúsquashes‚Äù it into range between 0 and 1\n",
    "\n",
    "  - large negative numbers become 0\n",
    "  - large positive numbers become 1\n",
    "\n",
    "- Has seen frequent use historically since it has a nice interpretation as the firing rate of a neuron\n",
    "\n",
    "  - Not firing at all (0) \n",
    "  - Fully-saturated firing at an assumed maximum frequency (1)\n",
    "\n",
    "- :thumbsdown:<span style=\"color:red\">Drawbacks</span>\n",
    "\n",
    "  - *Sigmoids saturate and kill gradients*\n",
    "\n",
    "    - Saturation: the gradient at either tail of 0 or 1 is almost zero\n",
    "\n",
    "      $\\rightarrow$ it will effectively ‚Äúkill‚Äù the gradient and almost no signal will flow through the neuron to its weights and recursively to its data.\n",
    "\n",
    "    - Must pay extra caution when initializing the weights of sigmoid neurons to prevent saturation\n",
    "\n",
    "  - *Sigmoid outputs are not zero-centered*.\n",
    "\n",
    "    - Undesirable zig-zagging dynamics in the gradient updates for the weights.\n",
    "      - If the data coming into a neuron is always positive (e.g. $x>0$ elementwise in $f=w^Tx+b$)), then the gradient on the weights $w$ will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression $f$).\n",
    "\n",
    "#### Tanh\n",
    "\n",
    "![img](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/tanh.jpeg)\n",
    "\n",
    "- Simply a scaled sigmoid:\n",
    "  $$\n",
    "  \\operatorname{tanh}(x) = 2\\sigma(2x) - 1\n",
    "  $$\n",
    "\n",
    "  - Squashes a real-valued number to $[-1, 1]$\n",
    "\n",
    "- Output is zero-centred :thumbsup: ($\\rightarrow$ always preferred to the sigmoid nonlinearity)\n",
    "\n",
    "- But its activation still saturates :cry:\n",
    "\n",
    "#### Recified Linear Unit (ReLU)\n",
    "\n",
    "![img](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/relu.jpeg)\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "- I.e., the activation is simply thresholded at zero\n",
    "- Pros :thumbsup:\n",
    "  - Greatly accelerate the convergence of stochastic gradient descent compared to the sigmoid/tanh functions\n",
    "  - Can be implemented by simply thresholding a matrix of activations at zero.\n",
    "- Cons :thumbsdown:\n",
    "  - ReLU units can be fragile during training and can ‚Äúdie‚Äù (Dead ReLU)\n",
    "    - A large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will *never* activate on any datapoint again.\n",
    "    - If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can **irreversibly die** during training since they can get knocked off the data manifold.\n",
    "    - More see: [What is the \"dying ReLU\" problem in neural networks?](https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks)\n",
    "\n",
    "#### Leaky ReLU\n",
    "\n",
    "- Attempt to fix the \"dying ReLU\" problem\n",
    "\n",
    "  - Have a small negative slope (of 0.01, or so) when $x < 0$\n",
    "    $$\n",
    "    f(x)=\\mathbf{1}(x<0)(\\alpha x)+\\mathbf{1}(x>=0)(x)\n",
    "    $$\n",
    "\n",
    "    - $\\alpha$: a small constant\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/1*siH_yCvYJ9rqWSUYeDBiRA.png\" alt=\"Activation Functions : Sigmoid, ReLU, Leaky ReLU and Softmax ...\" style=\"zoom: 33%;\" />\n",
    "\n",
    "- The consistency of the benefit across tasks is presently unclear :cry:\n",
    "\n",
    "#### Maxout\n",
    "\n",
    "$$\n",
    "\\max \\left(w_{1}^{T} x+b_{1}, w_{2}^{T} x+b_{2}\\right)\n",
    "$$\n",
    "\n",
    "- Both ReLU and Leaky ReLU are a special case of this form \n",
    "  - E.g., for ReLU we have $w_1, b_1 = 0$\n",
    "\n",
    "- Enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU)\n",
    "- Doubles the number of parameters for every single neuron, leading to a high total number of parameters :cry:\n",
    "\n",
    "#### Which activation function should I use?\n",
    "\n",
    "- Use the **ReLU** non-linearity\n",
    "  - Be careful with your learning rates\n",
    "  - Monitor the fraction of ‚Äúdead‚Äù units in a network\n",
    "\n",
    "- Hive **Leaky ReLU** or **Maxout** a try\n",
    "- *NEVER* use **sigmoid** :no_entry:\n",
    "\n",
    "- Try **tanh**\n",
    "  - Expect it to work worse than ReLU/Maxout\n",
    "\n",
    "\n",
    "\n",
    "## Neural Network architectures\n",
    "\n",
    "### Layer-wise organization\n",
    "\n",
    "üí° **Neural Networks as neurons in graphs**.\n",
    "\n",
    "- Model as collection of neurons that are connected in an *acyclic* graph\n",
    "\n",
    "  - I.e., outputs of some neurons can become inputs to other neurons\n",
    "\n",
    "- Often organized into distinct **layers** of neurons\n",
    "\n",
    "  - Most common layer type: **fully-connected layer**\n",
    "    - Neurons between two adjacent layers are *fully pairwise* connected, \n",
    "    - but neurons within a single layer share *no* connections\n",
    "\n",
    "- E.g.: \n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/neural_net2.jpeg\" alt=\"img\" style=\"zoom: 50%;\" />\n",
    "\n",
    "**Naming conventions**\n",
    "\n",
    "- When we say N-layer neural network, we do NOT count the input layer\n",
    "  - Single-layer neural network = a network with no hidden layers (input directly mapped to output)\n",
    "\n",
    "**Output layer**\n",
    "\n",
    "- The output layer neurons most commonly do NOT have an activation function\n",
    "  - Can also think of them as having a linear identity activation function\n",
    "- The last output layer is usually taken to\n",
    "  - represent the class scores (e.g. in classification), or \n",
    "  - some kind of real-valued target (e.g. in regression)\n",
    "\n",
    "**Sizing neural networks**\n",
    "\n",
    "- Two common metrics to measure the size of neural networks\n",
    "\n",
    "  - Number of neurons\n",
    "  - Number of parameters (more commonly used)\n",
    "\n",
    "- E.g., for the three-layer neural networks above:\n",
    "\n",
    "  - $4 + 4 + 1 = 9$ Neurons\n",
    "\n",
    "  - $(3 \\times 4) + (4 \\times 4) + (4 \\times 1) = 12 + 16 +4 = 32 $ weights and $4+4+1=9$ biases\n",
    "\n",
    "    $\\Rightarrow$ A total of $41$ learnable parameters\n",
    "\n",
    "\n",
    "\n",
    "### Example feed-forward computation\n",
    "\n",
    "**Why are Neural Networks organized into layers?**\n",
    "\n",
    "This structure makes it very simple and efficient to evaluate Neural Networks using *matrix vector operations*.\n",
    "\n",
    "For the three-layer neural networks above:\n",
    "\n",
    "- Input: $(3 \\times 1)$ vector\n",
    "- All connection strengths for a layer can be stored in a single matrix\n",
    "  - The first hidden layer\n",
    "    - weight: `W1` (size: ($4 \\times 3$))\n",
    "    - bias: `b1` (size: ($4 \\times 1$))\n",
    "    - every single neuron has its weights in a row of `W1`, so the matrix vector multiplication `np.dot(W1,x)` evaluates the activations of all neurons in that layer\n",
    "  - Similarly, `W2` would be a $(4 \\times 4)$ matrix that stores the connections of the second hidden layer\n",
    "  - `W3` would be a $(4 \\times 4)$ matrix for the last (output) layer.\n",
    "- The full forward pass of this 3-layer neural network is then **simply three matrix multiplications**, interwoven with the application of the activation function\n",
    "\n",
    "~~~python\n",
    "# forward-pass of a 3-layer neural network\n",
    "\n",
    "f = lambda x: 1.0 / (1.0 + np.exp(-x)) # activation function (sigmoid)\n",
    "x = np.random.randn(3, 1) # random input vector of 3 numbers (3 x 1)\n",
    "h1 = f(np.dot(W1, x) + b1) # first hidden layer (4 x 1)\n",
    "h2 = f(np.dot(W2, h1) + b2) # second hidden layer (4 x 1)\n",
    "out = np.dot(W3, h2) + b3 # output neuron (1 x 1)\n",
    "~~~\n",
    "\n",
    "- `W1`, `W2`, `W3`, `b1`, `b2`, `b3` are the learnable parameters of the network.\n",
    "- Instead of having a single input column vector, the variable `x` could hold an entire batch of training data (where each input example would be a column of `x`) and then all examples would be efficiently evaluated in parallel. \n",
    "\n",
    "### Representational power\n",
    "\n",
    "**Neural Networks with at least one hidden layer are *universal approximators***.\n",
    "\n",
    "- Given any continuous function $f(x)$ and some $\\epsilon > 0$ , there exists a Neural Network $g(x)$ with one hidden layer (with a reasonable choice of non-linearity), such that \n",
    "  $$\n",
    "  \\forall x: \\quad |f(x)-g(x)|<\\epsilon\n",
    "  $$\n",
    "  I.e.: the neural network can approximate any continuous function.\n",
    "\n",
    "In practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more.\n",
    "\n",
    "This is in stark contrast to **Convolutional Networks**, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers). \n",
    "\n",
    "- One argument for this observation is that images contain hierarchical structure (e.g. faces are made up of eyes, which are made up of edges, etc.), so several layers of processing make intuitive sense for this data domain.\n",
    "\n",
    "### Setting number of layers and their sizes\n",
    "\n",
    "- As we increase the size and number of layers in a Neural Network, the **capacity** of the network increases\n",
    "  - The space of representable functions grows since the neurons can collaborate to express many different functions\n",
    "  - Neural Networks with more neurons can express more complicated functions.\n",
    "    - :thumbsup: Pros: can learn to classify more complicated data\n",
    "    - :thumbsdown: Cons: easier to overfit the training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
