{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the data and the model\n",
    "\n",
    "## TL;DR\n",
    "\n",
    "- The recommended preprocessing is to center the data to have mean of zero, and normalize its scale to [-1, 1] along each feature\n",
    "- Initialize the weights by drawing them from a gaussian distribution with standard deviation of $\\sqrt(2/n)$, where $n$ is the number of inputs to the neuron. \n",
    "  - E.g. in numpy: `w = np.random.randn(n) * sqrt(2.0/n)`.\n",
    "- Use L2 regularization and dropout (the inverted version)\n",
    "- Use batch normalization\n",
    "\n",
    "\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "Data matrix: `X`, size `[N x D]`\n",
    "\n",
    "- `N`: number of data\n",
    "- `D`: dimensionality of data\n",
    "\n",
    "Three common forms of data preprocessing `X`:\n",
    "\n",
    "- Mean subtraction\n",
    "- Normalization\n",
    "- PCA and whitening\n",
    "\n",
    "### **Mean subtraction**\n",
    "\n",
    "- subtracting the mean across every individual *feature* in the data\n",
    "\n",
    "- geometric interpretation of *centering the cloud of data around the origin* along every dimension\n",
    "\n",
    "- Numpy implementation:\n",
    "\n",
    "  ```python\n",
    "  X -= np.mean(X, axis=0)\n",
    "  ```\n",
    "\n",
    "### **Normalization**\n",
    "\n",
    "- normalizing the data dimensions so that they are of approximately the *same scale*\n",
    "\n",
    "- Two ways:\n",
    "\n",
    "  - divide each dimension by its standard deviation, once it has been zero-centered\n",
    "\n",
    "    ~~~python\n",
    "    X /= np.std(X, axis=0)\n",
    "    ~~~\n",
    "\n",
    "  - normalizes each dimension so that the min and max along the dimension is -1 and 1 respectively\n",
    "\n",
    "- Makes sense to apply this preprocessing if you have a reason to believe that different input features have different scales (or units), but they should be of approximately equal importance to the learning algorithm.\n",
    "\n",
    "- In case of images, the relative scales of pixels are already approximately equal (and in range from 0 to 255)\n",
    "\n",
    "  $\\rightarrow$ It is not strictly necessary to perform this additional preprocessing step.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/prepro1.jpeg\" alt=\"img\" style=\"zoom:50%;\" />\n",
    "\n",
    "### **PCA and Whitenining**\n",
    "\n",
    "- The data is first centered as described above.\n",
    "\n",
    "- Then compute the covariance matrix that tells us about the correlation structure in the data\n",
    "\n",
    "  ~~~python\n",
    "  # Assume input data matrix X of size [N x D]\n",
    "  X -= np.mean(X, axis = 0) # zero-center the data (important)\n",
    "  cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix\n",
    "  ~~~\n",
    "\n",
    "  - The $(i, j)$ element of the data covariance matrix contains the *covariance* between i-th and j-th dimension of the data\n",
    "  - The diagonal of this matrix contains the variances\n",
    "  - The covariance matrix is symmetric and [positive semi-definite](http://en.wikipedia.org/wiki/Positive-definite_matrix#Negative-definite.2C_semidefinite_and_indefinite_matrices)\n",
    "\n",
    "- We can compute the SVD factorization of the data covariance matrix\n",
    "\n",
    "  ~~~python\n",
    "  U,S,V = np.linalg.svd(cov)\n",
    "  ~~~\n",
    "\n",
    "  - Columns of `U`: eigenvectors \n",
    "    - orthonormal vectors (norm of 1, orthogonal to each other)\n",
    "    - can be regarded as basis vectors\n",
    "  - `S`: 1-D array of the singular values\n",
    "\n",
    "- We project the original (but zero-centered) data into the eigenbasis\n",
    "\n",
    "  ~~~python\n",
    "  Xrot = np.dot(X, U)\n",
    "  ~~~\n",
    "\n",
    "  - `Xrot` is diagonal\n",
    "\n",
    "- In `np.linalg.svd`, its returned value `U`, the eigenvector columns are sorted by their eigenvalues\n",
    "\n",
    "  $\\rightarrow$ We can use this to reduce the dimensionality of the data by only using the top few eigenvectors, and discarding the dimensions along which the data has no variance.\n",
    "\n",
    "  - also refereed to as [Principal Component Analysis (PCA)](http://en.wikipedia.org/wiki/Principal_component_analysis) dimensionality reduction\n",
    "\n",
    "  ~~~python\n",
    "  Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100]\n",
    "  ~~~\n",
    "\n",
    "  - üëÜreduced the the original dataset of size [N x D] to one of size [N x 100], keeping the 100 dimensions of the data that contain the *most* variance\n",
    "  - can get very good performance by training linear classifiers or neural networks on the PCA-reduced datasets, obtaining savings in both space and time. üëè\n",
    "\n",
    "#### Whitening\n",
    "\n",
    "- takes the data in the eigenbasis\n",
    "- divides every dimension by the eigenvalue to normalize the scale\n",
    "- Geometric interpretation: if the input data is a multivariable gaussian, then the whitened data will be a gaussian with zero mean and identity covariance matrix\n",
    "\n",
    "~~~python\n",
    "# whiten the data:\n",
    "# divide by the eigenvalues (which are square roots of the singular values)\n",
    "Xwhite = Xrot / np.sqrt(S + 1e-5)\n",
    "~~~\n",
    "\n",
    "- `1e-5`: prevent division by zero\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/prepro2.jpeg\" alt=\"img\" style=\"zoom:50%;\" />\n",
    "\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "In practice, PCA/Whiteninig are NOT used with Convolutional Networks. üò≠\n",
    "\n",
    "However, it is very important to zero-center the data, and it is common to see normalization of every pixel as well.\n",
    "\n",
    "**Note**: \n",
    "\n",
    "An important point to make about the preprocessing is that any preprocessing statistics (e.g. the data mean) must **only be computed on the training data**, and then applied to the validation / test data. \n",
    "\n",
    "I.e.: the mean must be computed only over the training data and then subtracted equally from all splits (train/val/test).\n",
    "\n",
    "\n",
    "\n",
    "## Weight Initialization\n",
    "\n",
    "After constructing a Neural Network architecture and preprocessing the data,\n",
    "\n",
    "we have to initialize its parameters before we can begin to train the network.\n",
    "\n",
    "### Pitfall: all zero initialization ‚ùå\n",
    "\n",
    "NEVER set all the initial weights to zero!!!\n",
    "\n",
    "- if every neuron in the network computes the same output, then they will also all compute the same gradients during backpropagation and undergo the exact *same* parameter updates. \n",
    "- In other words, there is *no source of asymmetry* between neurons if their weights are initialized to be the same.\n",
    "\n",
    "### Small random numbers\n",
    "\n",
    "Initialize the weights of the neurons to **small numbers** and refer to doing so as *symmetry breaking*.\n",
    "\n",
    "- Idea: \n",
    "\n",
    "  - neurons are all random and unique in the beginning\n",
    "  - they will compute distinct updates and integrate themselves as diverse parts of the full network\n",
    "\n",
    "- E.g.: \n",
    "\n",
    "  ~~~python\n",
    "  W = 0.01 * np.random.randn(D, H)\n",
    "  ~~~\n",
    "\n",
    "  every neuron‚Äôs weight vector is initialized as a random vector sampled from a multi-dimensional gaussian\n",
    "\n",
    "**Warning**: It‚Äôs not necessarily the case that smaller numbers will work strictly better.\n",
    "\n",
    "### Calibrating the variances with 1/sqrt(n)\n",
    "\n",
    "Problem of small random numbers initialization:\n",
    "\n",
    "- the distribution of the outputs from a randomly initialized neuron has a variance that grows with the number of inputs\n",
    "\n",
    "Solution: normalize the variance of each neuron‚Äôs output to 1 by scaling its weight vector by the square root of its *fan-in* (i.e. its number of inputs).\n",
    "\n",
    "- Initialize each neuron's weight vector as\n",
    "\n",
    "  ~~~python\n",
    "  w = np.random.randn(n) / sqrt(n)\n",
    "  ~~~\n",
    "\n",
    "  - `n`: number of inputs\n",
    "\n",
    "- ensures that all neurons in the network initially have approximately the same output distribution\n",
    "\n",
    "  $\\rightarrow$ empirically improves the rate of convergence.\n",
    "\n",
    "### Sparse initialization\n",
    "\n",
    "- Set all weight matrices to zero, \n",
    "- but to break symmetry every neuron is randomly connected (with weights sampled from a small gaussian as above) to a fixed number of neurons below it.\n",
    "\n",
    "### Initializing the biases\n",
    "\n",
    "- Common: simply 0 bias initialization\n",
    "- For ReLU\n",
    "  - can use small constant value (such as 0.01) for all bias \n",
    "    - ensure all ReLU units fire in the beginning\n",
    "\n",
    "### Current recommendation in practice üí™\n",
    "\n",
    "- Use ReLU units\n",
    "\n",
    "- Initialize with \n",
    "\n",
    "  ~~~python\n",
    "  w = np.random.randn(n) * sqrt(2.0/n)\n",
    "  ~~~\n",
    "\n",
    "  (as discussed in [He et al.](http://arxiv-web3.library.cornell.edu/abs/1502.01852).)\n",
    "\n",
    "### Batch normalization\n",
    "\n",
    "- Explicitly force the activations throughout a network to take on a unit gaussian distribution at the beginning of the training\n",
    "- In practice usually insert the BatchNorm layer immediately after fully connected layers (or convolutional layers)\n",
    "- Use Batch Normalization are significantly more robust to bad initialization\n",
    "- Batch normalization can be interpreted as doing preprocessing at every layer of the network, but integrated into the network itself in a differentiable manner. \n",
    "\n",
    "## Regularization\n",
    "\n",
    "Controlling the capacity of Neural Networks to **prevent overfitting **(can be seen as penalizing some measure of complexity of the model).\n",
    "\n",
    "### L2 regularization\n",
    "\n",
    "- The most common form of regularization\n",
    "- Penalize the squared magnitude of all parameters directly in the objective\n",
    "  - For every weight $w$ in the network, add the term $\\frac{1}{2}\\lambda w^2$ to the objective\n",
    "    - $\\lambda$: regularization strength\n",
    "- Heavily penalizing peaky weight vectors and preferring diffuse weight vectors\n",
    "\n",
    "### L1 regularization\n",
    "\n",
    "- Another relatively common form of regularization\n",
    "- For each weight $w$ we add $\\lambda |w|$ to the objective\n",
    "- Leads the weight vectors to become sparse during optimization (i.e. very close to exactly zero)\n",
    "\n",
    "  - I.e., neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the ‚Äúnoisy‚Äù inputs\n",
    "- We can combine L1 regularization with L2 regularization: $\\lambda_1 |w| + \\lambda_2 w^2$ (a.k.a  [Elastic net regularization](http://web.stanford.edu/~hastie/Papers/B67.2 (2005) 301-320 Zou & Hastie.pdf))\n",
    "\n",
    "### Max norm constraints\n",
    "\n",
    "- Enforce an absolute upper bound on the magnitude of the weight vector for every neuron\n",
    "- Use projected gradient descent to enforce the constraint\n",
    "- In practice, this corresponds to \n",
    "  - performing the parameter update as normal\n",
    "  - then enforcing the constraint by clamping the weight vector $w$ of every neuron to satisfy $\\|w\\|_2 < c$\n",
    "\n",
    "- The network cannot ‚Äúexplode‚Äù even when the learning rates are set too high because the updates are always bounded.\n",
    "\n",
    "### Dropout üëç\n",
    "\n",
    "- Extremely effective, simple regularization technique\n",
    "\n",
    "- Complements the other methods (L1, L2, maxnorm)\n",
    "\n",
    "- Only keep a neuron active with some probability $p$ (a hyperparameter), or setting it to zero otherwise\n",
    "\n",
    "- Can be interpreted as sampling a Neural Network within the full Neural Network, and only updating the parameters of the sampled network based on the input data. \n",
    "\n",
    "  - Note: the exponential number of possible sampled networks are *NOT independent* because they share the parameters\n",
    "\n",
    "- During testing there is NO dropout applied, with the interpretation of evaluating an averaged prediction across the exponentially-sized ensemble of all sub-networks (similar to ensemble learning)\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/dropout.jpeg\" alt=\"img\" style=\"zoom:80%;\" />\n",
    "\n",
    "Example: Vanilla dropout in an 3-layer Neural Network:\n",
    "\n",
    "~~~python\n",
    "\"\"\" Vanilla Dropout: Not recommended implementation \"\"\"\n",
    "\n",
    "p = 0.5 # probability of keep a unit active (higher p = less dropout)\n",
    "\n",
    "def train_step(X):\n",
    "  \"\"\" X contains the data\"\"\"\n",
    "  \n",
    "  # forward pass \n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1)\n",
    "  U1 = np.random.rand(*H1.shape) < p # first dropout mask\n",
    "  H1 *= U1 # drop\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "  U2 = np.random.rand(*H2.shape) < p # second dropout mask\n",
    "  H2 *= U2 # drop\n",
    "  out = np.dot(W3, H2) + b3\n",
    "  \n",
    "  # backward pass: compute gradients... (not shown)\n",
    "  # perform parameter update... (not shown)\n",
    "  \n",
    "def predict(X):\n",
    "  # ensembled forward pass\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # scale the activations\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # scale the activations\n",
    "  out = np.dot(W3, H2) + b3\n",
    "~~~\n",
    "\n",
    "Note: In the `predict` function we are not dropping anymore, but we are performing a **scaling of both hidden layer outputs** by $p$. \n",
    "\n",
    "- This is important because at test time all neurons see all their inputs, so we want the outputs of neurons at test time to be *identical* to their expected outputs at training time.\n",
    "\n",
    "- E.g., consider an output of a neuron $x$ (before dropout)\n",
    "\n",
    "  - With dropout, the expected output is\n",
    "    $$\n",
    "    px + (1-p)0\n",
    "    $$\n",
    "\n",
    "  - At test time, when we keep the neuron always active, we must adjust $x \\to px$ to keep the same expected output\n",
    "\n",
    "- Performing this attenuation at test time can be related to the process of iterating over all the possible binary masks (and therefore all the exponentially many sub-networks) and computing their ensemble prediction.\n",
    "\n",
    "The undesirable property of the scheme presented above is that **we must scale the activations by $p$ at test time.** Since test-time performance is so critical, it is always preferable to use **inverted dropout**, which \n",
    "\n",
    "- performs the scaling at train time, leaving the forward pass at test time untouched.\n",
    "- üëç Additional pros: prediction code can remain untouched when you decide to tweak where you apply dropout, or if at all.\n",
    "\n",
    "~~~python\n",
    "\"\"\" \n",
    "Inverted Dropout: Recommended implementation example.\n",
    "We drop and scale at train time and don't do anything at test time.\n",
    "\"\"\"\n",
    "\n",
    "p = 0.5 # probability of keeping a unit active. higher = less dropout\n",
    "\n",
    "def train_step(X):\n",
    "  # forward pass for example 3-layer neural network\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1)\n",
    "  U1 = (np.random.rand(*H1.shape) < p) / p # first dropout mask. Notice /p!\n",
    "  H1 *= U1 # drop!\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "  U2 = (np.random.rand(*H2.shape) < p) / p # second dropout mask. Notice /p!\n",
    "  H2 *= U2 # drop!\n",
    "  out = np.dot(W3, H2) + b3\n",
    "  \n",
    "  # backward pass: compute gradients... (not shown)\n",
    "  # perform parameter update... (not shown)\n",
    "  \n",
    "def predict(X):\n",
    "  # ensembled forward pass\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2) # no scaling necessary\n",
    "  out = np.dot(W3, H2) + b3\n",
    "~~~\n",
    "\n",
    "### **Theme of noise in forward pass**\n",
    "\n",
    "- A random set of weights is instead set to zero during forward pass ([DropConnect](http://cs.nyu.edu/~wanli/dropc/))\n",
    "- Convolutional Neural Networks also take advantage of this theme with methods such as stochastic pooling, fractional pooling, and data augmentation. \n",
    "\n",
    "### **Bias regularization**\n",
    "\n",
    "In practical applications (and with proper data preprocessing) regularizing the bias rarely leads to significantly worse performance. This is likely because there are very few bias terms compared to all the weights, so the classifier can ‚Äúafford to‚Äù use the biases if it needs them to obtain a better data loss.\n",
    "\n",
    "### Summary\n",
    "\n",
    "In practice:\n",
    "\n",
    "- Most common to use a single, global L2 regularization strength that is cross-validated.\n",
    "- Also common to combine this with dropout applied after all layers. \n",
    "  - The value of $p=0.5$ is a reasonable default, but this can be tuned on validation data.\n",
    "\n",
    "\n",
    "\n",
    "## Loss functions\n",
    "\n",
    "In a supervised learning problem, ***data loss* measures the compatibility between a prediction (e.g. the class scores in classification) and the ground truth label.**\n",
    "\n",
    "- Takes the form of an average over the data losses for every individual example\n",
    "  $$\n",
    "  L = \\frac{1}{N}\\sum_i L_i\n",
    "  $$\n",
    "\n",
    "  - $N$: number of training data\n",
    "\n",
    " Lets abbreviate $f = f(x_i; W)$ to be the activations of the output layer in a Neural Network.\n",
    "\n",
    "### Classification\n",
    "\n",
    "We assume a dataset of examples and a single correct label (out of a fixed set) for each example. \n",
    "\n",
    "Two common functions:\n",
    "\n",
    "- SVM\n",
    "\n",
    "  $$\n",
    "  \\max \\left(0, f_{j}-f_{y_{i}}+1\\right)^{2}\n",
    "  $$\n",
    "\n",
    "  - some people report better performance with the squared hinge loss (using $\\max \\left(0, f_{j}-f_{y_{i}}+1\\right)^{2}$)\n",
    "\n",
    "-  Softmax classifier that uses the cross-entropy loss\n",
    "\n",
    "   $$\n",
    "   L_{i}=-\\log \\left(\\frac{e^{f_{y_{i}}}}{\\sum_{j} e^{f_{j}}}\\right)\n",
    "   $$\n",
    "\n",
    "   - Problem: large number of classes\n",
    "     - When the set of labels is very large (e.g. words in English dictionary, or ImageNet which contains 22,000 categories), computing the full softmax probabilities becomes expensive. \n",
    "\n",
    "### Regression\n",
    "\n",
    "The task of predicting real-valued quantities.\n",
    "\n",
    "For this task, it is common to compute the loss between the predicted quantity and the true answer and then measure \n",
    "\n",
    "- the L2 squared norm\n",
    "  $$\n",
    "  L_{i}=\\left\\|f-y_{i}\\right\\|_{2}^{2}\n",
    "  $$\n",
    "\n",
    "- or L1 norm of the difference (formulated by summing the absolute value along each dimension)\n",
    "  $$\n",
    "  L_{i}=\\left\\|f-y_{i}\\right\\|_{1}=\\sum_{j}\\left|f_{j}-\\left(y_{i}\\right)_{j}\\right|\n",
    "  $$\n",
    "\n",
    "### Word of caution\n",
    "\n",
    "**The L2 loss is much harder to optimize than a more stable loss, such as Softmax.**\n",
    "\n",
    "- The L2 loss requires a very fragile and specific property from the network to output exactly one correct value for each input (and its augmentations)\n",
    "  - This is not the case with Softmax, where the precise value of each score is less important: *It only matters that their magnitudes are appropriate*.\n",
    "\n",
    "- The L2 loss is less robust because outliers can introduce huge gradients.\n",
    "- Applying dropout in the network (especially in the layer right before the L2 loss) is not a great idea"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
